{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Pytorch的GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据处理部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入标签，并将其转换为独热编码的numpy列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稀疏矩阵转换为稀疏张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建图（graph）的邻接矩阵（adjacency matrix），并通过处理节点索引和边的数据，生成一个稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将稀疏矩阵标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(dim = 1)[1].type_as(labels)#取每行最大值所对应的标签\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、图卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "# Parse arguments (use empty args for Jupyter compatibility)\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# Move to CUDA if available\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9404 acc_train: 0.1286 loss_val: 1.9241 acc_val: 0.1000 time: 0.4684s\n",
      "Epoch: 0002 loss_train: 1.9310 acc_train: 0.1357 loss_val: 1.9112 acc_val: 0.1133 time: 0.0068s\n",
      "Epoch: 0003 loss_train: 1.9095 acc_train: 0.2286 loss_val: 1.8990 acc_val: 0.3500 time: 0.0058s\n",
      "Epoch: 0004 loss_train: 1.8992 acc_train: 0.2500 loss_val: 1.8873 acc_val: 0.3500 time: 0.0058s\n",
      "Epoch: 0005 loss_train: 1.8872 acc_train: 0.2857 loss_val: 1.8760 acc_val: 0.3500 time: 0.0058s\n",
      "Epoch: 0006 loss_train: 1.8807 acc_train: 0.3000 loss_val: 1.8655 acc_val: 0.3500 time: 0.0060s\n",
      "Epoch: 0007 loss_train: 1.8656 acc_train: 0.2929 loss_val: 1.8558 acc_val: 0.3500 time: 0.0062s\n",
      "Epoch: 0008 loss_train: 1.8599 acc_train: 0.2929 loss_val: 1.8466 acc_val: 0.3500 time: 0.0063s\n",
      "Epoch: 0009 loss_train: 1.8469 acc_train: 0.2929 loss_val: 1.8376 acc_val: 0.3500 time: 0.0063s\n",
      "Epoch: 0010 loss_train: 1.8396 acc_train: 0.2929 loss_val: 1.8289 acc_val: 0.3500 time: 0.0064s\n",
      "Epoch: 0011 loss_train: 1.8276 acc_train: 0.2929 loss_val: 1.8204 acc_val: 0.3500 time: 0.0066s\n",
      "Epoch: 0012 loss_train: 1.8261 acc_train: 0.2929 loss_val: 1.8121 acc_val: 0.3500 time: 0.0066s\n",
      "Epoch: 0013 loss_train: 1.8183 acc_train: 0.2929 loss_val: 1.8041 acc_val: 0.3500 time: 0.0067s\n",
      "Epoch: 0014 loss_train: 1.8099 acc_train: 0.2929 loss_val: 1.7963 acc_val: 0.3500 time: 0.0068s\n",
      "Epoch: 0015 loss_train: 1.7999 acc_train: 0.2929 loss_val: 1.7887 acc_val: 0.3500 time: 0.0068s\n",
      "Epoch: 0016 loss_train: 1.7768 acc_train: 0.2929 loss_val: 1.7814 acc_val: 0.3500 time: 0.0068s\n",
      "Epoch: 0017 loss_train: 1.7755 acc_train: 0.2929 loss_val: 1.7742 acc_val: 0.3500 time: 0.0071s\n",
      "Epoch: 0018 loss_train: 1.7571 acc_train: 0.2929 loss_val: 1.7672 acc_val: 0.3500 time: 0.0072s\n",
      "Epoch: 0019 loss_train: 1.7595 acc_train: 0.2929 loss_val: 1.7603 acc_val: 0.3500 time: 0.0072s\n",
      "Epoch: 0020 loss_train: 1.7649 acc_train: 0.2929 loss_val: 1.7537 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0021 loss_train: 1.7556 acc_train: 0.2929 loss_val: 1.7475 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0022 loss_train: 1.7470 acc_train: 0.2929 loss_val: 1.7417 acc_val: 0.3500 time: 0.0072s\n",
      "Epoch: 0023 loss_train: 1.7353 acc_train: 0.3000 loss_val: 1.7361 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0024 loss_train: 1.7143 acc_train: 0.2929 loss_val: 1.7305 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0025 loss_train: 1.7122 acc_train: 0.3000 loss_val: 1.7250 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0026 loss_train: 1.7336 acc_train: 0.3000 loss_val: 1.7194 acc_val: 0.3500 time: 0.0074s\n",
      "Epoch: 0027 loss_train: 1.7074 acc_train: 0.2929 loss_val: 1.7137 acc_val: 0.3500 time: 0.0074s\n",
      "Epoch: 0028 loss_train: 1.6980 acc_train: 0.3000 loss_val: 1.7078 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0029 loss_train: 1.6899 acc_train: 0.3000 loss_val: 1.7018 acc_val: 0.3500 time: 0.0074s\n",
      "Epoch: 0030 loss_train: 1.6771 acc_train: 0.3429 loss_val: 1.6954 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0031 loss_train: 1.6730 acc_train: 0.3143 loss_val: 1.6887 acc_val: 0.3500 time: 0.0077s\n",
      "Epoch: 0032 loss_train: 1.6418 acc_train: 0.3071 loss_val: 1.6819 acc_val: 0.3533 time: 0.0077s\n",
      "Epoch: 0033 loss_train: 1.6587 acc_train: 0.3286 loss_val: 1.6748 acc_val: 0.3533 time: 0.0077s\n",
      "Epoch: 0034 loss_train: 1.6629 acc_train: 0.3286 loss_val: 1.6675 acc_val: 0.3600 time: 0.0076s\n",
      "Epoch: 0035 loss_train: 1.6221 acc_train: 0.3571 loss_val: 1.6599 acc_val: 0.3633 time: 0.0077s\n",
      "Epoch: 0036 loss_train: 1.6535 acc_train: 0.3214 loss_val: 1.6521 acc_val: 0.3633 time: 0.0076s\n",
      "Epoch: 0037 loss_train: 1.6333 acc_train: 0.3786 loss_val: 1.6443 acc_val: 0.3633 time: 0.0077s\n",
      "Epoch: 0038 loss_train: 1.6080 acc_train: 0.3929 loss_val: 1.6361 acc_val: 0.3700 time: 0.0077s\n",
      "Epoch: 0039 loss_train: 1.5892 acc_train: 0.3643 loss_val: 1.6276 acc_val: 0.3700 time: 0.0077s\n",
      "Epoch: 0040 loss_train: 1.5668 acc_train: 0.3786 loss_val: 1.6188 acc_val: 0.3767 time: 0.0077s\n",
      "Epoch: 0041 loss_train: 1.5514 acc_train: 0.4071 loss_val: 1.6095 acc_val: 0.3767 time: 0.0076s\n",
      "Epoch: 0042 loss_train: 1.5583 acc_train: 0.3929 loss_val: 1.5997 acc_val: 0.3800 time: 0.0076s\n",
      "Epoch: 0043 loss_train: 1.5313 acc_train: 0.4214 loss_val: 1.5896 acc_val: 0.3867 time: 0.0077s\n",
      "Epoch: 0044 loss_train: 1.5146 acc_train: 0.4214 loss_val: 1.5791 acc_val: 0.3900 time: 0.0076s\n",
      "Epoch: 0045 loss_train: 1.4814 acc_train: 0.4500 loss_val: 1.5687 acc_val: 0.4000 time: 0.0077s\n",
      "Epoch: 0046 loss_train: 1.4778 acc_train: 0.4357 loss_val: 1.5580 acc_val: 0.4067 time: 0.0076s\n",
      "Epoch: 0047 loss_train: 1.4576 acc_train: 0.4286 loss_val: 1.5472 acc_val: 0.4067 time: 0.0076s\n",
      "Epoch: 0048 loss_train: 1.4476 acc_train: 0.4143 loss_val: 1.5361 acc_val: 0.4067 time: 0.0076s\n",
      "Epoch: 0049 loss_train: 1.4461 acc_train: 0.4357 loss_val: 1.5249 acc_val: 0.4100 time: 0.0078s\n",
      "Epoch: 0050 loss_train: 1.4319 acc_train: 0.4571 loss_val: 1.5135 acc_val: 0.4100 time: 0.0076s\n",
      "Epoch: 0051 loss_train: 1.4180 acc_train: 0.4571 loss_val: 1.5019 acc_val: 0.4133 time: 0.0076s\n",
      "Epoch: 0052 loss_train: 1.4100 acc_train: 0.4500 loss_val: 1.4904 acc_val: 0.4200 time: 0.0076s\n",
      "Epoch: 0053 loss_train: 1.3897 acc_train: 0.4786 loss_val: 1.4788 acc_val: 0.4333 time: 0.0076s\n",
      "Epoch: 0054 loss_train: 1.3656 acc_train: 0.4786 loss_val: 1.4670 acc_val: 0.4433 time: 0.0076s\n",
      "Epoch: 0055 loss_train: 1.3513 acc_train: 0.5286 loss_val: 1.4549 acc_val: 0.4667 time: 0.0078s\n",
      "Epoch: 0056 loss_train: 1.3439 acc_train: 0.5500 loss_val: 1.4428 acc_val: 0.4933 time: 0.0088s\n",
      "Epoch: 0057 loss_train: 1.3047 acc_train: 0.5429 loss_val: 1.4306 acc_val: 0.5067 time: 0.0080s\n",
      "Epoch: 0058 loss_train: 1.2984 acc_train: 0.5786 loss_val: 1.4183 acc_val: 0.5233 time: 0.0078s\n",
      "Epoch: 0059 loss_train: 1.2535 acc_train: 0.6071 loss_val: 1.4059 acc_val: 0.5433 time: 0.0078s\n",
      "Epoch: 0060 loss_train: 1.2806 acc_train: 0.5714 loss_val: 1.3931 acc_val: 0.5433 time: 0.0078s\n",
      "Epoch: 0061 loss_train: 1.2572 acc_train: 0.6214 loss_val: 1.3803 acc_val: 0.5533 time: 0.0079s\n",
      "Epoch: 0062 loss_train: 1.2420 acc_train: 0.6143 loss_val: 1.3674 acc_val: 0.5667 time: 0.0078s\n",
      "Epoch: 0063 loss_train: 1.2349 acc_train: 0.5857 loss_val: 1.3547 acc_val: 0.5833 time: 0.0078s\n",
      "Epoch: 0064 loss_train: 1.1993 acc_train: 0.6929 loss_val: 1.3416 acc_val: 0.6000 time: 0.0078s\n",
      "Epoch: 0065 loss_train: 1.1988 acc_train: 0.6286 loss_val: 1.3285 acc_val: 0.6100 time: 0.0078s\n",
      "Epoch: 0066 loss_train: 1.1638 acc_train: 0.6857 loss_val: 1.3156 acc_val: 0.6400 time: 0.0078s\n",
      "Epoch: 0067 loss_train: 1.1312 acc_train: 0.7286 loss_val: 1.3024 acc_val: 0.6700 time: 0.0078s\n",
      "Epoch: 0068 loss_train: 1.1576 acc_train: 0.6929 loss_val: 1.2892 acc_val: 0.6767 time: 0.0078s\n",
      "Epoch: 0069 loss_train: 1.1687 acc_train: 0.7071 loss_val: 1.2761 acc_val: 0.6833 time: 0.0079s\n",
      "Epoch: 0070 loss_train: 1.1406 acc_train: 0.6929 loss_val: 1.2634 acc_val: 0.6933 time: 0.0078s\n",
      "Epoch: 0071 loss_train: 1.1222 acc_train: 0.7500 loss_val: 1.2501 acc_val: 0.7100 time: 0.0078s\n",
      "Epoch: 0072 loss_train: 1.0827 acc_train: 0.7643 loss_val: 1.2367 acc_val: 0.7200 time: 0.0078s\n",
      "Epoch: 0073 loss_train: 1.0789 acc_train: 0.7786 loss_val: 1.2231 acc_val: 0.7267 time: 0.0078s\n",
      "Epoch: 0074 loss_train: 1.0603 acc_train: 0.7571 loss_val: 1.2099 acc_val: 0.7267 time: 0.0078s\n",
      "Epoch: 0075 loss_train: 1.0598 acc_train: 0.7929 loss_val: 1.1973 acc_val: 0.7333 time: 0.0078s\n",
      "Epoch: 0076 loss_train: 1.0003 acc_train: 0.7571 loss_val: 1.1855 acc_val: 0.7400 time: 0.0079s\n",
      "Epoch: 0077 loss_train: 0.9849 acc_train: 0.8143 loss_val: 1.1742 acc_val: 0.7467 time: 0.0078s\n",
      "Epoch: 0078 loss_train: 0.9951 acc_train: 0.7143 loss_val: 1.1633 acc_val: 0.7633 time: 0.0079s\n",
      "Epoch: 0079 loss_train: 1.0536 acc_train: 0.7286 loss_val: 1.1528 acc_val: 0.7700 time: 0.0078s\n",
      "Epoch: 0080 loss_train: 0.9777 acc_train: 0.8000 loss_val: 1.1423 acc_val: 0.7733 time: 0.0078s\n",
      "Epoch: 0081 loss_train: 0.9968 acc_train: 0.7857 loss_val: 1.1310 acc_val: 0.7767 time: 0.0078s\n",
      "Epoch: 0082 loss_train: 0.9499 acc_train: 0.8000 loss_val: 1.1195 acc_val: 0.7767 time: 0.0093s\n",
      "Epoch: 0083 loss_train: 0.8969 acc_train: 0.8071 loss_val: 1.1086 acc_val: 0.7700 time: 0.0087s\n",
      "Epoch: 0084 loss_train: 0.9376 acc_train: 0.7857 loss_val: 1.0980 acc_val: 0.7667 time: 0.0080s\n",
      "Epoch: 0085 loss_train: 0.9066 acc_train: 0.8000 loss_val: 1.0881 acc_val: 0.7733 time: 0.0080s\n",
      "Epoch: 0086 loss_train: 0.8950 acc_train: 0.7643 loss_val: 1.0788 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0087 loss_train: 0.9330 acc_train: 0.8214 loss_val: 1.0696 acc_val: 0.7867 time: 0.0081s\n",
      "Epoch: 0088 loss_train: 0.9184 acc_train: 0.8000 loss_val: 1.0608 acc_val: 0.7800 time: 0.0103s\n",
      "Epoch: 0089 loss_train: 0.8696 acc_train: 0.8214 loss_val: 1.0524 acc_val: 0.7867 time: 0.0080s\n",
      "Epoch: 0090 loss_train: 0.8307 acc_train: 0.8286 loss_val: 1.0436 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0091 loss_train: 0.8255 acc_train: 0.8714 loss_val: 1.0351 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0092 loss_train: 0.8283 acc_train: 0.8714 loss_val: 1.0263 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0093 loss_train: 0.7673 acc_train: 0.8429 loss_val: 1.0173 acc_val: 0.7867 time: 0.0080s\n",
      "Epoch: 0094 loss_train: 0.8455 acc_train: 0.8429 loss_val: 1.0081 acc_val: 0.7833 time: 0.0079s\n",
      "Epoch: 0095 loss_train: 0.8430 acc_train: 0.7857 loss_val: 0.9997 acc_val: 0.7933 time: 0.0079s\n",
      "Epoch: 0096 loss_train: 0.7907 acc_train: 0.8286 loss_val: 0.9923 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0097 loss_train: 0.8268 acc_train: 0.8643 loss_val: 0.9853 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0098 loss_train: 0.7811 acc_train: 0.8357 loss_val: 0.9791 acc_val: 0.7900 time: 0.0093s\n",
      "Epoch: 0099 loss_train: 0.7377 acc_train: 0.8643 loss_val: 0.9734 acc_val: 0.7867 time: 0.0086s\n",
      "Epoch: 0100 loss_train: 0.7418 acc_train: 0.8714 loss_val: 0.9682 acc_val: 0.7800 time: 0.0082s\n",
      "Epoch: 0101 loss_train: 0.7565 acc_train: 0.8429 loss_val: 0.9629 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0102 loss_train: 0.7202 acc_train: 0.8500 loss_val: 0.9572 acc_val: 0.7767 time: 0.0081s\n",
      "Epoch: 0103 loss_train: 0.7609 acc_train: 0.8286 loss_val: 0.9513 acc_val: 0.7700 time: 0.0086s\n",
      "Epoch: 0104 loss_train: 0.7897 acc_train: 0.8357 loss_val: 0.9449 acc_val: 0.7733 time: 0.0081s\n",
      "Epoch: 0105 loss_train: 0.7562 acc_train: 0.8571 loss_val: 0.9384 acc_val: 0.7733 time: 0.0089s\n",
      "Epoch: 0106 loss_train: 0.7619 acc_train: 0.8357 loss_val: 0.9309 acc_val: 0.7767 time: 0.0093s\n",
      "Epoch: 0107 loss_train: 0.7390 acc_train: 0.8357 loss_val: 0.9241 acc_val: 0.7733 time: 0.0083s\n",
      "Epoch: 0108 loss_train: 0.6837 acc_train: 0.8571 loss_val: 0.9172 acc_val: 0.7767 time: 0.0084s\n",
      "Epoch: 0109 loss_train: 0.6888 acc_train: 0.8714 loss_val: 0.9110 acc_val: 0.7833 time: 0.0083s\n",
      "Epoch: 0110 loss_train: 0.6871 acc_train: 0.8643 loss_val: 0.9065 acc_val: 0.7767 time: 0.0083s\n",
      "Epoch: 0111 loss_train: 0.7027 acc_train: 0.8714 loss_val: 0.9017 acc_val: 0.7733 time: 0.0098s\n",
      "Epoch: 0112 loss_train: 0.6741 acc_train: 0.8357 loss_val: 0.8971 acc_val: 0.7733 time: 0.0081s\n",
      "Epoch: 0113 loss_train: 0.6910 acc_train: 0.8643 loss_val: 0.8935 acc_val: 0.7733 time: 0.0080s\n",
      "Epoch: 0114 loss_train: 0.7052 acc_train: 0.8643 loss_val: 0.8902 acc_val: 0.7733 time: 0.0079s\n",
      "Epoch: 0115 loss_train: 0.6464 acc_train: 0.8643 loss_val: 0.8871 acc_val: 0.7767 time: 0.0079s\n",
      "Epoch: 0116 loss_train: 0.7106 acc_train: 0.8357 loss_val: 0.8828 acc_val: 0.7767 time: 0.0079s\n",
      "Epoch: 0117 loss_train: 0.6564 acc_train: 0.8786 loss_val: 0.8778 acc_val: 0.7767 time: 0.0079s\n",
      "Epoch: 0118 loss_train: 0.6026 acc_train: 0.9214 loss_val: 0.8721 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0119 loss_train: 0.6237 acc_train: 0.8929 loss_val: 0.8668 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0120 loss_train: 0.6881 acc_train: 0.8571 loss_val: 0.8626 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0121 loss_train: 0.6582 acc_train: 0.8786 loss_val: 0.8579 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0122 loss_train: 0.6361 acc_train: 0.8714 loss_val: 0.8530 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0123 loss_train: 0.6776 acc_train: 0.8714 loss_val: 0.8479 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0124 loss_train: 0.6403 acc_train: 0.9071 loss_val: 0.8434 acc_val: 0.7767 time: 0.0081s\n",
      "Epoch: 0125 loss_train: 0.6242 acc_train: 0.8357 loss_val: 0.8399 acc_val: 0.7767 time: 0.0079s\n",
      "Epoch: 0126 loss_train: 0.6181 acc_train: 0.8857 loss_val: 0.8368 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0127 loss_train: 0.6054 acc_train: 0.8929 loss_val: 0.8338 acc_val: 0.7767 time: 0.0081s\n",
      "Epoch: 0128 loss_train: 0.6025 acc_train: 0.8929 loss_val: 0.8310 acc_val: 0.7733 time: 0.0112s\n",
      "Epoch: 0129 loss_train: 0.6323 acc_train: 0.8571 loss_val: 0.8276 acc_val: 0.7733 time: 0.0091s\n",
      "Epoch: 0130 loss_train: 0.6033 acc_train: 0.8857 loss_val: 0.8240 acc_val: 0.7733 time: 0.0092s\n",
      "Epoch: 0131 loss_train: 0.5896 acc_train: 0.8643 loss_val: 0.8207 acc_val: 0.7733 time: 0.0079s\n",
      "Epoch: 0132 loss_train: 0.6394 acc_train: 0.9000 loss_val: 0.8177 acc_val: 0.7800 time: 0.0079s\n",
      "Epoch: 0133 loss_train: 0.6054 acc_train: 0.8857 loss_val: 0.8154 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0134 loss_train: 0.6027 acc_train: 0.8643 loss_val: 0.8137 acc_val: 0.7833 time: 0.0079s\n",
      "Epoch: 0135 loss_train: 0.5922 acc_train: 0.9000 loss_val: 0.8120 acc_val: 0.7833 time: 0.0079s\n",
      "Epoch: 0136 loss_train: 0.5940 acc_train: 0.8857 loss_val: 0.8100 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0137 loss_train: 0.5926 acc_train: 0.8929 loss_val: 0.8079 acc_val: 0.7833 time: 0.0079s\n",
      "Epoch: 0138 loss_train: 0.5645 acc_train: 0.9071 loss_val: 0.8051 acc_val: 0.7833 time: 0.0079s\n",
      "Epoch: 0139 loss_train: 0.6022 acc_train: 0.8643 loss_val: 0.8023 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.5606 acc_train: 0.8929 loss_val: 0.7999 acc_val: 0.7767 time: 0.0079s\n",
      "Epoch: 0141 loss_train: 0.5741 acc_train: 0.8857 loss_val: 0.7979 acc_val: 0.7767 time: 0.0079s\n",
      "Epoch: 0142 loss_train: 0.5294 acc_train: 0.9000 loss_val: 0.7960 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0143 loss_train: 0.5536 acc_train: 0.8929 loss_val: 0.7948 acc_val: 0.7800 time: 0.0079s\n",
      "Epoch: 0144 loss_train: 0.5944 acc_train: 0.8929 loss_val: 0.7936 acc_val: 0.7833 time: 0.0079s\n",
      "Epoch: 0145 loss_train: 0.6038 acc_train: 0.8643 loss_val: 0.7903 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0146 loss_train: 0.5266 acc_train: 0.9143 loss_val: 0.7873 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0147 loss_train: 0.5388 acc_train: 0.9000 loss_val: 0.7841 acc_val: 0.7800 time: 0.0079s\n",
      "Epoch: 0148 loss_train: 0.5315 acc_train: 0.8857 loss_val: 0.7814 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0149 loss_train: 0.5462 acc_train: 0.9143 loss_val: 0.7795 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0150 loss_train: 0.5498 acc_train: 0.8786 loss_val: 0.7776 acc_val: 0.7900 time: 0.0079s\n",
      "Epoch: 0151 loss_train: 0.5924 acc_train: 0.8786 loss_val: 0.7760 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0152 loss_train: 0.5163 acc_train: 0.9143 loss_val: 0.7753 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0153 loss_train: 0.5211 acc_train: 0.8643 loss_val: 0.7750 acc_val: 0.7900 time: 0.0080s\n",
      "Epoch: 0154 loss_train: 0.5111 acc_train: 0.9286 loss_val: 0.7741 acc_val: 0.7967 time: 0.0079s\n",
      "Epoch: 0155 loss_train: 0.5273 acc_train: 0.8857 loss_val: 0.7718 acc_val: 0.7967 time: 0.0079s\n",
      "Epoch: 0156 loss_train: 0.5058 acc_train: 0.9143 loss_val: 0.7693 acc_val: 0.7967 time: 0.0078s\n",
      "Epoch: 0157 loss_train: 0.5243 acc_train: 0.9286 loss_val: 0.7663 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0158 loss_train: 0.4969 acc_train: 0.9143 loss_val: 0.7631 acc_val: 0.7900 time: 0.0081s\n",
      "Epoch: 0159 loss_train: 0.5153 acc_train: 0.8929 loss_val: 0.7605 acc_val: 0.7867 time: 0.0081s\n",
      "Epoch: 0160 loss_train: 0.5282 acc_train: 0.8786 loss_val: 0.7585 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0161 loss_train: 0.4930 acc_train: 0.9000 loss_val: 0.7565 acc_val: 0.7800 time: 0.0079s\n",
      "Epoch: 0162 loss_train: 0.4897 acc_train: 0.9000 loss_val: 0.7548 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0163 loss_train: 0.5384 acc_train: 0.8786 loss_val: 0.7535 acc_val: 0.7800 time: 0.0079s\n",
      "Epoch: 0164 loss_train: 0.4987 acc_train: 0.9143 loss_val: 0.7518 acc_val: 0.7867 time: 0.0079s\n",
      "Epoch: 0165 loss_train: 0.4663 acc_train: 0.8929 loss_val: 0.7497 acc_val: 0.7900 time: 0.0079s\n",
      "Epoch: 0166 loss_train: 0.5300 acc_train: 0.8929 loss_val: 0.7480 acc_val: 0.7933 time: 0.0079s\n",
      "Epoch: 0167 loss_train: 0.5158 acc_train: 0.9071 loss_val: 0.7469 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0168 loss_train: 0.4909 acc_train: 0.9214 loss_val: 0.7452 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0169 loss_train: 0.5116 acc_train: 0.9000 loss_val: 0.7435 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0170 loss_train: 0.5081 acc_train: 0.9143 loss_val: 0.7421 acc_val: 0.7967 time: 0.0079s\n",
      "Epoch: 0171 loss_train: 0.4482 acc_train: 0.9143 loss_val: 0.7420 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0172 loss_train: 0.5291 acc_train: 0.9000 loss_val: 0.7422 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0173 loss_train: 0.5284 acc_train: 0.9286 loss_val: 0.7420 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0174 loss_train: 0.4877 acc_train: 0.9071 loss_val: 0.7411 acc_val: 0.8000 time: 0.0081s\n",
      "Epoch: 0175 loss_train: 0.4626 acc_train: 0.9214 loss_val: 0.7406 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0176 loss_train: 0.4393 acc_train: 0.9214 loss_val: 0.7410 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0177 loss_train: 0.5101 acc_train: 0.9071 loss_val: 0.7416 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0178 loss_train: 0.4760 acc_train: 0.9286 loss_val: 0.7402 acc_val: 0.8000 time: 0.0079s\n",
      "Epoch: 0179 loss_train: 0.5131 acc_train: 0.9214 loss_val: 0.7382 acc_val: 0.7967 time: 0.0079s\n",
      "Epoch: 0180 loss_train: 0.4575 acc_train: 0.9000 loss_val: 0.7355 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0181 loss_train: 0.4860 acc_train: 0.9286 loss_val: 0.7324 acc_val: 0.7967 time: 0.0088s\n",
      "Epoch: 0182 loss_train: 0.4522 acc_train: 0.9429 loss_val: 0.7294 acc_val: 0.7967 time: 0.0086s\n",
      "Epoch: 0183 loss_train: 0.4626 acc_train: 0.9286 loss_val: 0.7272 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0184 loss_train: 0.4591 acc_train: 0.9143 loss_val: 0.7251 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0185 loss_train: 0.4933 acc_train: 0.9071 loss_val: 0.7234 acc_val: 0.7933 time: 0.0079s\n",
      "Epoch: 0186 loss_train: 0.4604 acc_train: 0.9429 loss_val: 0.7225 acc_val: 0.7900 time: 0.0080s\n",
      "Epoch: 0187 loss_train: 0.4979 acc_train: 0.9000 loss_val: 0.7217 acc_val: 0.7900 time: 0.0080s\n",
      "Epoch: 0188 loss_train: 0.4976 acc_train: 0.9000 loss_val: 0.7228 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0189 loss_train: 0.4688 acc_train: 0.9214 loss_val: 0.7248 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0190 loss_train: 0.4708 acc_train: 0.9143 loss_val: 0.7252 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0191 loss_train: 0.4416 acc_train: 0.9214 loss_val: 0.7247 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0192 loss_train: 0.4554 acc_train: 0.8929 loss_val: 0.7229 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0193 loss_train: 0.4493 acc_train: 0.9214 loss_val: 0.7204 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0194 loss_train: 0.4361 acc_train: 0.9357 loss_val: 0.7182 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0195 loss_train: 0.4519 acc_train: 0.9143 loss_val: 0.7159 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0196 loss_train: 0.4739 acc_train: 0.9214 loss_val: 0.7153 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0197 loss_train: 0.4383 acc_train: 0.9214 loss_val: 0.7149 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0198 loss_train: 0.4350 acc_train: 0.9357 loss_val: 0.7125 acc_val: 0.8000 time: 0.0079s\n",
      "Epoch: 0199 loss_train: 0.4230 acc_train: 0.9071 loss_val: 0.7114 acc_val: 0.7967 time: 0.0079s\n",
      "Epoch: 0200 loss_train: 0.4432 acc_train: 0.9286 loss_val: 0.7100 acc_val: 0.7933 time: 0.0079s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.0554s\n",
      "Test set results: loss= 0.7390 accuracy= 0.8060\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
